{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TED-Talks\" data-toc-modified-id=\"TED-Talks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>TED Talks</a></span></li><li><span><a href=\"#Токенизация\" data-toc-modified-id=\"Токенизация-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Токенизация</a></span></li><li><span><a href=\"#Распределение-слов-по-частоте,-стоп-слова\" data-toc-modified-id=\"Распределение-слов-по-частоте,-стоп-слова-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Распределение слов по частоте, стоп слова</a></span></li><li><span><a href=\"#Приведение-к-нормальной-форме\" data-toc-modified-id=\"Приведение-к-нормальной-форме-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Приведение к нормальной форме</a></span></li><li><span><a href=\"#Выделение-коллокаций\" data-toc-modified-id=\"Выделение-коллокаций-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Выделение коллокаций</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>TF-IDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#поиск-похожих\" data-toc-modified-id=\"поиск-похожих-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>поиск похожих</a></span></li><li><span><a href=\"#выделение-ключевых-слов\" data-toc-modified-id=\"выделение-ключевых-слов-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>выделение ключевых слов</a></span></li></ul></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Word2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentence-Embeddings\" data-toc-modified-id=\"Sentence-Embeddings-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Sentence Embeddings</a></span></li></ul></li><li><span><a href=\"#Topic-Modeling\" data-toc-modified-id=\"Topic-Modeling-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Topic Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Поиск-похожих-документов\" data-toc-modified-id=\"Поиск-похожих-документов-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Поиск похожих документов</a></span></li><li><span><a href=\"#Краткое-описание-документов-(суммаризация)\" data-toc-modified-id=\"Краткое-описание-документов-(суммаризация)-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Краткое описание документов (суммаризация)</a></span></li><li><span><a href=\"#Визуализация-тематической-модели\" data-toc-modified-id=\"Визуализация-тематической-модели-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Визуализация тематической модели</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install razdel\n",
    "# !pip install pyLDAvis\n",
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:47:05.497424Z",
     "start_time": "2020-11-10T10:47:00.207503Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import razdel\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pymorphy2\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import plotly\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:47:05.503628Z",
     "start_time": "2020-11-10T10:47:05.499673Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "TOKEN_PATTERN = \"[а-яё]+\"\n",
    "\n",
    "DATA_PATH = './data/ted_talks.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you are using colab\n",
    "# !mkdir ./data\n",
    "# !wget https://raw.githubusercontent.com/shestakoff/sphere-ml-intro/master/2020/lecture07-nlp/data/ted_talks.csv.gz -O $DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TED Talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:40.957377Z",
     "start_time": "2020-11-10T10:48:40.365424Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:43.146907Z",
     "start_time": "2020-11-10T10:48:43.142074Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:44.069551Z",
     "start_time": "2020-11-10T10:48:43.856545Z"
    }
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"ww9ClmCWBr0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:47.739358Z",
     "start_time": "2020-11-10T10:48:47.288428Z"
    }
   },
   "outputs": [],
   "source": [
    "df.text.str.len().hist(bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:50.347446Z",
     "start_time": "2020-11-10T10:48:49.872868Z"
    }
   },
   "outputs": [],
   "source": [
    "min_talk_len = 750\n",
    "\n",
    "df = df[df.text.str.len() >= min_talk_len].reset_index(drop=True)\n",
    "\n",
    "df.text.str.len().hist(bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:52.071504Z",
     "start_time": "2020-11-10T10:48:52.067850Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:52.739688Z",
     "start_time": "2020-11-10T10:48:52.736786Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = df.text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:57.325788Z",
     "start_time": "2020-11-10T10:48:57.321367Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"Как же так?! Олег... Мы же в 18.00 договаривались встретиться:(\"\n",
    "\n",
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:58.296138Z",
     "start_time": "2020-11-10T10:48:58.292327Z"
    }
   },
   "outputs": [],
   "source": [
    "print(re.findall(\"[а-яА-ЯёЁ]+\", sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:58.810071Z",
     "start_time": "2020-11-10T10:48:58.806074Z"
    }
   },
   "outputs": [],
   "source": [
    "print(re.split(r\"[-\\s.,;!?]+\", sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:59.438413Z",
     "start_time": "2020-11-10T10:48:59.434419Z"
    }
   },
   "outputs": [],
   "source": [
    "print([token.text for token in razdel.tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:48:59.982377Z",
     "start_time": "2020-11-10T10:48:59.978817Z"
    }
   },
   "outputs": [],
   "source": [
    "print(nltk.tokenize.casual_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:49:03.079778Z",
     "start_time": "2020-11-10T10:49:03.075401Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[0][:255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:49:17.424896Z",
     "start_time": "2020-11-10T10:49:16.340634Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(TOKEN_PATTERN, text.lower())\n",
    "\n",
    "docs = [tokenize(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:49:18.084120Z",
     "start_time": "2020-11-10T10:49:18.070954Z"
    }
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Распределение слов по частоте, стоп слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте словарь `слово` -> `количество вхождений в корпус`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:49:44.569760Z",
     "start_time": "2020-11-10T10:49:43.120516Z"
    }
   },
   "outputs": [],
   "source": [
    "occurence = Counter()\n",
    "\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        occurence[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:49:47.786316Z",
     "start_time": "2020-11-10T10:49:47.782229Z"
    }
   },
   "outputs": [],
   "source": [
    "len(occurence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:57:09.967303Z",
     "start_time": "2020-10-31T13:57:09.956743Z"
    }
   },
   "source": [
    "в словаре больше 150 тыс. слов, хотя в литературном русском около 150 тыс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:49:52.262385Z",
     "start_time": "2020-11-10T10:49:52.225315Z"
    }
   },
   "outputs": [],
   "source": [
    "occurence.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самые частые слова относятся к служебным частям речи и несут мало полезной информации. Их можно отбрасывать с помощью подготовленных списков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:50:32.746656Z",
     "start_time": "2020-11-10T10:50:32.740731Z"
    }
   },
   "outputs": [],
   "source": [
    "stopword_set = set(nltk.corpus.stopwords.words('russian'))\n",
    "# stopword_set = stopword_set.union({'это', 'который', 'весь', 'наш', 'свой', 'ещё', 'её', 'ваш', 'также', 'итак'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:50:34.040004Z",
     "start_time": "2020-11-10T10:50:34.018910Z"
    }
   },
   "outputs": [],
   "source": [
    "stopword_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:50:41.318702Z",
     "start_time": "2020-11-10T10:50:40.300663Z"
    }
   },
   "outputs": [],
   "source": [
    "max_rank = 40_000\n",
    "\n",
    "counts = [count for _, count in occurence.most_common()[:max_rank]]\n",
    "\n",
    "plt.plot(range(1, len(counts) + 1), counts)\n",
    "plt.title('log-log scale', fontsize=18)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('rank', fontsize=16)\n",
    "plt.ylabel('count', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./zipf_law.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Приведение к нормальной форме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:50:48.788987Z",
     "start_time": "2020-11-10T10:50:48.621193Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_tokens = sorted(occurence.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:50:53.625016Z",
     "start_time": "2020-11-10T10:50:53.611639Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в словаре встречаются разные формы одного и того же слова:\n",
    "* 'аббревиатур',\n",
    "* 'аббревиатура',\n",
    "* 'аббревиатуре',\n",
    "* 'аббревиатурой',\n",
    "* 'аббревиатуру',\n",
    "* 'аббревиатуры'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:51:10.524774Z",
     "start_time": "2020-11-10T10:51:10.522028Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:51:12.653299Z",
     "start_time": "2020-11-10T10:51:12.649609Z"
    }
   },
   "outputs": [],
   "source": [
    "words = [\n",
    "    'аббревиатур',\n",
    "    'аббревиатура',\n",
    "    'аббревиатуре',\n",
    "    'аббревиатурой',\n",
    "    'аббревиатуру',\n",
    "    'аббревиатуры',\n",
    "    'человек',\n",
    "    'люди',\n",
    "    'людьми'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:51:13.749140Z",
     "start_time": "2020-11-10T10:51:13.744028Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:51:16.730110Z",
     "start_time": "2020-11-10T10:51:16.630551Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:51:17.130639Z",
     "start_time": "2020-11-10T10:51:17.125432Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer.parse('стекло')[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:51:21.453564Z",
     "start_time": "2020-11-10T10:51:21.448175Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(lemmatizer.parse(word)[0].normal_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:51:24.751673Z",
     "start_time": "2020-11-10T10:51:24.744146Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer.parse('хендай')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:51:45.755754Z",
     "start_time": "2020-11-10T10:51:45.751836Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer.word_is_known('хендай')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T10:54:10.390976Z",
     "start_time": "2020-11-10T10:52:55.560678Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer_cache = {}\n",
    "\n",
    "def lemmatize(token):\n",
    "    if lemmatizer.word_is_known(token):\n",
    "        if token not in lemmatizer_cache:\n",
    "            lemmatizer_cache[token] = lemmatizer.parse(token)[0].normal_form\n",
    "        return lemmatizer_cache[token]\n",
    "    return token\n",
    "\n",
    "lemmatized_docs = [[lemmatize(token) for token in text] for text in tqdm_notebook(docs)]\n",
    "\n",
    "cleared_docs = [[token for token in text if token not in stopword_set] for text in lemmatized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:20:27.733110Z",
     "start_time": "2020-11-10T11:20:27.671980Z"
    }
   },
   "outputs": [],
   "source": [
    "cleared_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во сколько раз уменьшился словарь после лемматизации?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:20:35.511692Z",
     "start_time": "2020-11-10T11:20:34.635618Z"
    }
   },
   "outputs": [],
   "source": [
    "len({token for text in cleared_docs for token in text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделение коллокаций\n",
    "\n",
    "Посчитайте, сколько раз пары слов встречались вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:20:46.617389Z",
     "start_time": "2020-11-10T11:20:42.450072Z"
    }
   },
   "outputs": [],
   "source": [
    "cooccurence = Counter()\n",
    "\n",
    "for doc in docs:\n",
    "    for i in range(len(doc) - 1):\n",
    "        cooccurence[(doc[i], doc[i + 1])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:20:47.894565Z",
     "start_time": "2020-11-10T11:20:47.709993Z"
    }
   },
   "outputs": [],
   "source": [
    "cooccurence.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "PMI(x,y) = \\log\\frac{p\\left(x,y\\right)}{p\\left(x\\right)p\\left(y\\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(w_i) \\sim \\frac{\\text{count}\\left(w_i\\right)}{\\displaystyle\\sum_{j=1}^n \\text{count}\\left( w_j \\right)} = \\frac{\\text{count}\\left(w_i\\right)}{N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PMI(w_1, w_2) \\sim \\log\\left(\\frac{N \\cdot \\text{count}\\left(w_1, w_2\\right)}{\\text{count}\\left(w_1\\right) \\text{count}\\left(w_2\\right) }\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T18:51:18.654645Z",
     "start_time": "2020-11-02T18:51:18.608034Z"
    }
   },
   "source": [
    "постройте словарь, в котором парам слов соответствует значения pmi\n",
    "\n",
    "добавляйте в словарь только те пары слов, которые встретились в корпусе хотя бы `min_cooccur` раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:20:54.055761Z",
     "start_time": "2020-11-10T11:20:53.867923Z"
    }
   },
   "outputs": [],
   "source": [
    "cooccurence.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:20:54.779252Z",
     "start_time": "2020-11-10T11:20:54.732962Z"
    }
   },
   "outputs": [],
   "source": [
    "occurence.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:02.809990Z",
     "start_time": "2020-11-10T11:21:02.801546Z"
    }
   },
   "outputs": [],
   "source": [
    "N = sum(occurence.values())\n",
    "\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:13.591659Z",
     "start_time": "2020-11-10T11:21:12.887623Z"
    }
   },
   "outputs": [],
   "source": [
    "pmi = Counter()\n",
    "min_cooccur = 20\n",
    "\n",
    "for pair in cooccurence:\n",
    "    if cooccurence[pair] >= min_cooccur:\n",
    "        pmi[pair] = np.log(cooccurence[pair] * N / occurence[pair[0]] / occurence[pair[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:14.212347Z",
     "start_time": "2020-11-10T11:21:14.138289Z"
    }
   },
   "outputs": [],
   "source": [
    "pmi.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "если хочется автоматически выделять коллокации для извлечения признаков: [gensim.models.phrases.{Phrases, Phraser}](https://www.machinelearningplus.com/nlp/gensim-tutorial/#10howtocreatebigramsandtrigramsusingphrasermodels)\n",
    "\n",
    "```python\n",
    "phrases = (bows, min_count=30, progress_per=500)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(phrases)\n",
    "\n",
    "bigram[bows[0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## поиск похожих"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:24.479201Z",
     "start_time": "2020-11-10T11:21:23.519437Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=lambda x: x, lowercase=False, max_df=0.8, min_df=2,\n",
    "    norm='l2'\n",
    ").fit(cleared_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "получите tf-idf векторы документов, посчитайте cosine similarity каждого документа с каждым"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:27.530178Z",
     "start_time": "2020-11-10T11:21:26.537281Z"
    }
   },
   "outputs": [],
   "source": [
    "docs_tf_idf = vectorizer.transform(cleared_docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:29.102613Z",
     "start_time": "2020-11-10T11:21:28.274061Z"
    }
   },
   "outputs": [],
   "source": [
    "similarities = docs_tf_idf @ docs_tf_idf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:31.978715Z",
     "start_time": "2020-11-10T11:21:31.161215Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "\n",
    "for i in range(similarities.shape[0]):\n",
    "    for j in range(similarities.shape[1]):\n",
    "        if j > i:\n",
    "            if similarities[i, j] > threshold:\n",
    "                print(f\"({i}, {j}) {similarities[i, j]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:35.983728Z",
     "start_time": "2020-11-10T11:21:35.976213Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_talks(talk_ids, text_chunk=1024):\n",
    "\n",
    "    for talk_id in talk_ids:\n",
    "        record = df.loc[talk_id]\n",
    "\n",
    "        print(record.talk)\n",
    "        print('-' * 100)\n",
    "        print(record.text[:text_chunk])\n",
    "        print('#' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:36.595869Z",
     "start_time": "2020-11-10T11:21:36.527249Z"
    }
   },
   "outputs": [],
   "source": [
    "print_talks((815, 1199))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## выделение ключевых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:42.785781Z",
     "start_time": "2020-11-10T11:21:42.781081Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_id = 815"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "выделите `top_k` ключевых слов для документа `doc_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:45.604440Z",
     "start_time": "2020-11-10T11:21:45.560773Z"
    }
   },
   "outputs": [],
   "source": [
    "features = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:51.105500Z",
     "start_time": "2020-11-10T11:21:51.099538Z"
    }
   },
   "outputs": [],
   "source": [
    "order = docs_tf_idf[doc_id].argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:21:52.851456Z",
     "start_time": "2020-11-10T11:21:52.706542Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'feture': features[order], 'tf-idf': docs_tf_idf[doc_id, order]}).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:23:21.686078Z",
     "start_time": "2020-11-10T11:22:13.843101Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_sentence_dataset(documents):\n",
    "    tokenized_sentences = []\n",
    "    for document in tqdm_notebook(documents):\n",
    "        for sentence in razdel.sentenize(document):\n",
    "            lemmatized_tokens = [lemmatize(token) for token in re.findall(TOKEN_PATTERN, sentence.text.lower())]\n",
    "            tokenized_sentences.append(\n",
    "                [token for token in lemmatized_tokens if token not in stopword_set]\n",
    "            )\n",
    "    return tokenized_sentences\n",
    "\n",
    "sentence_dataset = prepare_sentence_dataset(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:23:21.692227Z",
     "start_time": "2020-11-10T11:23:21.687668Z"
    }
   },
   "outputs": [],
   "source": [
    "len(sentence_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:23:21.697615Z",
     "start_time": "2020-11-10T11:23:21.693742Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:23:21.704218Z",
     "start_time": "2020-11-10T11:23:21.699244Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec(\n",
    "    size=100, sg=0, window=5, min_count=5, negative=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:23:25.555514Z",
     "start_time": "2020-11-10T11:23:21.705708Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec.build_vocab(sentence_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:23:25.561935Z",
     "start_time": "2020-11-10T11:23:25.557637Z"
    }
   },
   "outputs": [],
   "source": [
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:24:40.843456Z",
     "start_time": "2020-11-10T11:23:25.563821Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec.train(sentence_dataset, total_examples=word2vec.corpus_count, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:24:40.886207Z",
     "start_time": "2020-11-10T11:24:40.845876Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec.wv.most_similar('мама')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:24:40.933070Z",
     "start_time": "2020-11-10T11:24:40.889124Z"
    }
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "test_words = ['альтруизм', 'бедность', 'платье', 'сентябрь', 'компьютер', 'лондон']\n",
    "\n",
    "for word in test_words:\n",
    "    print(word)\n",
    "    print(\n",
    "        tabulate(word2vec.wv.most_similar(word), tablefmt='orgtbl', headers=('neighbor', 'score')),\n",
    "        end='\\n\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:24:40.947276Z",
     "start_time": "2020-11-10T11:24:40.935400Z"
    }
   },
   "outputs": [],
   "source": [
    "index2word = np.array(word2vec.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:24:40.957311Z",
     "start_time": "2020-11-10T11:24:40.949758Z"
    }
   },
   "outputs": [],
   "source": [
    "index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:24:40.962258Z",
     "start_time": "2020-11-10T11:24:40.959694Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = word2vec.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:24:40.969584Z",
     "start_time": "2020-11-10T11:24:40.964802Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обучите TSNE на выборке случайных слов для понижения размерности векторов до двух компонент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:24:40.975725Z",
     "start_time": "2020-11-10T11:24:40.972391Z"
    }
   },
   "outputs": [],
   "source": [
    "ids = np.random.randint(low=0, high=index2word.size, size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:24:59.002945Z",
     "start_time": "2020-11-10T11:24:40.978535Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_reduced = TSNE(random_state=SEED, n_components=2).fit_transform(embeddings[ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:24:59.012497Z",
     "start_time": "2020-11-10T11:24:59.005140Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_tsne_embeddings(embeddings, annotations):\n",
    "\n",
    "    trace = plotly.graph_objs.Scattergl(\n",
    "        x = embeddings[:, 0],\n",
    "        y = embeddings[:, 1],\n",
    "        name = 'Embedding',\n",
    "        mode = 'markers',\n",
    "\n",
    "        marker = dict(\n",
    "            colorscale='Viridis',\n",
    "            size = 6,\n",
    "            line = dict(width = 0.5),\n",
    "            opacity=0.75\n",
    "        ),\n",
    "        text=annotations\n",
    "    )\n",
    "\n",
    "    layout = dict(\n",
    "        title = \"Word2Vec 2D TSNE Embeddings\",\n",
    "        yaxis = dict(zeroline = False),\n",
    "        xaxis = dict(zeroline = False),\n",
    "        hovermode = 'closest',\n",
    "        width=800,\n",
    "        height=800\n",
    "    )\n",
    "\n",
    "    return plotly.graph_objs.Figure(data=[trace], layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:00.862211Z",
     "start_time": "2020-11-10T11:24:59.015187Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_tsne_embeddings(embeddings_reduced, index2word[ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T13:34:05.038696Z",
     "start_time": "2020-10-31T13:34:05.025442Z"
    }
   },
   "source": [
    "## Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "будем строить эмбеддинг предложения простым средним векторов слов. какие еще возможны варианты?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:00.866176Z",
     "start_time": "2020-11-10T11:25:00.863626Z"
    }
   },
   "outputs": [],
   "source": [
    "def embed_text(text, word2index, word_embeddings):\n",
    "    return np.array([\n",
    "        word_embeddings[word2index[word]] for word in text \n",
    "        if word in word2index and word not in stopword_set\n",
    "    ]).mean(0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:00.878581Z",
     "start_time": "2020-11-10T11:25:00.867667Z"
    }
   },
   "outputs": [],
   "source": [
    "word2index = {word: i for i, word in enumerate(index2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:02.930170Z",
     "start_time": "2020-11-10T11:25:00.880262Z"
    }
   },
   "outputs": [],
   "source": [
    "talk2vec = np.concatenate([embed_text(doc, word2index, embeddings) for doc in cleared_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:02.935728Z",
     "start_time": "2020-11-10T11:25:02.932527Z"
    }
   },
   "outputs": [],
   "source": [
    "normed_talk2vec = normalize(talk2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:02.958833Z",
     "start_time": "2020-11-10T11:25:02.937322Z"
    }
   },
   "outputs": [],
   "source": [
    "similarities = normed_talk2vec @ normed_talk2vec.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:06.283892Z",
     "start_time": "2020-11-10T11:25:02.960805Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.96\n",
    "\n",
    "for i in range(similarities.shape[0]):\n",
    "    for j in range(similarities.shape[1]):\n",
    "        if j > i:\n",
    "            if similarities[i, j] > threshold:\n",
    "                print(f\"({i}, {j}) {similarities[i, j]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:06.291338Z",
     "start_time": "2020-11-10T11:25:06.285764Z"
    }
   },
   "outputs": [],
   "source": [
    "print_talks((78, 116))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:06.298225Z",
     "start_time": "2020-11-10T11:25:06.293336Z"
    }
   },
   "outputs": [],
   "source": [
    "print_talks((276, 1713))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:22.050056Z",
     "start_time": "2020-11-10T11:25:06.299911Z"
    }
   },
   "outputs": [],
   "source": [
    "talk2vec_reduced = TSNE(n_components=2, random_state=SEED).fit_transform(talk2vec)\n",
    "\n",
    "plot_tsne_embeddings(talk2vec_reduced, df.talk.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:24.160615Z",
     "start_time": "2020-11-10T11:25:22.052946Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(cleared_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:24.166014Z",
     "start_time": "2020-11-10T11:25:24.162344Z"
    }
   },
   "outputs": [],
   "source": [
    "len(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:24.183901Z",
     "start_time": "2020-11-10T11:25:24.167680Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = dictionary[0] # так надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:25.393278Z",
     "start_time": "2020-11-10T11:25:24.186804Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in cleared_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:25:25.413694Z",
     "start_time": "2020-11-10T11:25:25.395369Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:42:17.155609Z",
     "start_time": "2020-11-10T11:40:27.096963Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary.id2token,\n",
    "    chunksize=200,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=400,\n",
    "    num_topics=20,\n",
    "    passes=20,\n",
    "    random_state=SEED,\n",
    "    per_word_topics=True,\n",
    "    eval_every=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:44:47.334131Z",
     "start_time": "2020-11-10T11:44:47.261442Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица тем:  $$\\Theta \\left( T \\times \\vert V \\vert \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:46:16.891513Z",
     "start_time": "2020-11-10T11:46:16.875165Z"
    }
   },
   "outputs": [],
   "source": [
    "theta = lda_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:46:17.468950Z",
     "start_time": "2020-11-10T11:46:17.454353Z"
    }
   },
   "outputs": [],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица документов: $$\\Phi \\left(\\vert D \\vert \\times  T \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:46:18.592087Z",
     "start_time": "2020-11-10T11:46:18.580006Z"
    }
   },
   "outputs": [],
   "source": [
    "document_topics = lda_model.get_document_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:46:23.375434Z",
     "start_time": "2020-11-10T11:46:19.362884Z"
    }
   },
   "outputs": [],
   "source": [
    "phi = gensim.matutils.corpus2dense(document_topics, num_terms=lda_model.num_topics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:46:24.368375Z",
     "start_time": "2020-11-10T11:46:24.365238Z"
    }
   },
   "outputs": [],
   "source": [
    "phi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск похожих документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:46:25.817031Z",
     "start_time": "2020-11-10T11:46:25.766555Z"
    }
   },
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(phi, phi)\n",
    "\n",
    "similarities.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:46:31.244346Z",
     "start_time": "2020-11-10T11:46:30.703200Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df.text.apply(lambda x: 'машинное обучение' in x.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:46:54.813648Z",
     "start_time": "2020-11-10T11:46:54.805544Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_id = 540\n",
    "top_k = 20\n",
    "\n",
    "sorted_doc_ids = similarities[doc_id].argsort()[::-1][:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:46:57.130558Z",
     "start_time": "2020-11-10T11:46:57.076629Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "pd.DataFrame({'neighbor': df.talk.values[sorted_doc_ids], 'similarity': similarities[doc_id, sorted_doc_ids]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Краткое описание документов (суммаризация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:47:38.271775Z",
     "start_time": "2020-11-10T11:47:38.239877Z"
    }
   },
   "outputs": [],
   "source": [
    "def summarize(document, model, top_k):\n",
    "    tokenized_sentences = []\n",
    "    sentences = []\n",
    "    for sentence in razdel.sentenize(document):\n",
    "        sentences.append(sentence.text)\n",
    "        lemmatized_tokens = [lemmatize(token) for token in re.findall(TOKEN_PATTERN, sentence.text.lower())]\n",
    "        tokenized_sentences.append(\n",
    "            [token for token in lemmatized_tokens if token not in stopword_set]\n",
    "        )\n",
    "    \n",
    "    tokenized_document = [token for sentence in tokenized_sentences for token in sentence]\n",
    "    bow = dictionary.doc2bow(tokenized_document)\n",
    "    document_topics = [model.get_document_topics(bow)]\n",
    "    document_vector = gensim.matutils.corpus2dense(document_topics, num_terms=model.num_topics).reshape(1, -1)\n",
    "    \n",
    "    sentence_bows = [dictionary.doc2bow(sentence) for sentence in tokenized_sentences]\n",
    "    sentence_topics = model.get_document_topics(sentence_bows)\n",
    "    sentence_vectors = gensim.matutils.corpus2dense(sentence_topics, num_terms=model.num_topics).T\n",
    "    \n",
    "    relevance = cosine_similarity(document_vector, sentence_vectors)[0]\n",
    "    most_relevant_sentences = relevance.argsort()[::-1][:top_k]\n",
    "    \n",
    "    return \" \".join([sentence for i, sentence in enumerate(sentences) if i in most_relevant_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:47:41.838597Z",
     "start_time": "2020-11-10T11:47:41.463093Z"
    }
   },
   "outputs": [],
   "source": [
    "summarize(df.text.values[540], lda_model, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:47:49.720006Z",
     "start_time": "2020-11-10T11:47:49.714421Z"
    }
   },
   "outputs": [],
   "source": [
    "df.text.values[540]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация тематической модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T11:48:43.219471Z",
     "start_time": "2020-11-10T11:47:53.276990Z"
    }
   },
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
